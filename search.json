[{"title":"ElasticSearch 底层探究 —— Lucene 的理解","url":"/2021/01/01/lucene/","content":"\n\n## 前言\n\n在平常的工作项目中，频繁用到了Elasticsearch这个搜索引擎，包括日志查询，大文本查询，地理空间查询等等场景。之前也专门学习过 ES 的结构，知道每个ES节点包含多个Shard，一个shard是一个Lucene index ，Shard中包含一个个Segment，每个Segment 都是一个完整的倒排索引。\n\n原本是想好好研究下Lucene的原理， 但是在读ES的官方文档的时候，看到上面有这么一段描述\n\n>  Elasticsearch is an open-source search engine built on top of Apache Lucene™, a full-text search-engine library. Lucene is arguably the most advanced, high-performance, and fully featured search engine library in existence today—​both open source and proprietary.\n>  \n>  But Lucene is just a library. To leverage its power, you need to work in Java and to integrate Lucene directly with your application.**Worse, you will likely require a degree in information retrieval to understand how it works. Lucene is very complex.**\n\n好家伙，官方都说Lucene复杂到难以理解，甚至需要读个信息学相关学位，那我还是不去深究了吧，反正能用就ok了嘛！抱着这样的想法，我放弃了继续探究的打算。\n\n然而，随着业务越来越复杂，流量规模越来越大，ES数次出现了高负载， 卡顿甚至宕机的问题，也试着做了好多次优化，但效果并不是特别理想。虽然加钱加机器能解决很大一部分问题，但作为技术人员，还是想要以技术手段切入。\n\n于是，我不得不硬着头皮去学习Lucene原理了。我查询了许多相关资料甚至论文，结果出乎意料的是，Lucene并不是真的十分晦涩难懂，认真理解理解，还是比较容易明白个大概的。本文便是我个人关于Lucene的认知。\n\n## 一些基础概念\n\n### 倒排索引和全文检索\n\n 所谓倒排索引（Inverted index），对应的是传统的正排索引(forward index)，或者叫顺序查找。\n \n 以一篇论文为例，文章由各种单词和符号构成，正排索引是站在文章的角度，记录每个文档（ID）都含有哪些单词，以及每个单词出现了多少次（词频）及其出现位置（偏移量）；而倒排索引是站在单词角度看文档，标识每个单词分别在那些文档中出现(ID)，以及在各自的文档中每个单词的词频和偏移量。\n \n 倒排索引的建立过程就是先用待查询内容构建文档，然后将文档进行分词，词干提取，转小写，去重等一系列操作后组织成词汇表，存储起来以待查询。\n \n 由此不难看出倒排索引的优缺点，优点是大大增加的查询效率和准确率，尤其适合海量数据和大文本的查询。相应的，缺点就是占用了更多的储存空间，属于典型的空间换时间类型。\n \n而全文检索就是以倒排索引为基础，利用预先建立好的索引，针对文档中的字段进行搜索的过程，效率远高于数据库的模糊查找。\n \n ### 分词与分词器\n\n#### 分词原理\n\n建立索引和查询的过程中，都是以基本的语素项为单位的。基本的语素项就是通过分词得到，这个过程决定了索引单元金额最终的匹配过程。分词在文本索引的建立过程和用户提交检索过程中都存在。利用相同的分词器，把短语或者句子切分成相同的结果，才能保证检索过程顺利进行。\n\n英文分词的原理基本的处理流程是：输入文本、词汇分割、词汇过滤（去除停留词）、词干提取（形态还原）、大写转为小写、结果输出。\n\n中文分词原理中文分词比较复杂，并没有英文分词那么简单。这主要是因为中文的词与词之间并不像英文中那样用空格来隔开。主要的方法有三种：基于词典匹配的分词方法、基于语义理解的分词、基于词频统计的分词。\n1. 基于字典匹配的分词方法，按照一定的匹配策略将输入的字符串与机器字典词条进行匹配。如果在词典中找到当前字符串，则匹配成功输出识别的词汇。\n按照匹配操作的 扫描方向不同，字典匹配分词方法可以分为正向匹配和逆向匹配，以及结合了两者的双向匹配算法\n按照不同长度优先匹配的情况，可以分为最大（最长）匹配和最 小（最短）匹配\n按照是否与词性标注过程相结合，又可以分为单纯分词方法和分词与词性标注相结合的方法\n几种常用的词典分词方法如下所示：\n    * 正向最大匹配（由左到右的方向）\n    * 逆向最大匹配（由右到左的方向）\n    * 最少切分（是每一句中切除的词数最小）\n\n2. 基于语义理解的分词，模拟人脑对语言和句子的理解，达到识别词汇单元的效果。基本模式是把分词、句法、语义分析并行进行，利用句法和语义信息来处理分词的歧义。一般结构中通常包括分词子系统、句法语义子系统、调度系统。\n\n3. 基于词频统计的分词，这种做法基于人们对中文词语的直接感觉。通常词是稳定的词的组合，因此在中文文章的上下文中，相邻的字搭配出现的频率越多，就越有可能形成一个固定的词。\n\n实际应用的统计分词系统都使用一个基本的常用词词典，把字典分词和统计分词结合使用。基于统计的方法能很好 地解决词典未收录新词的处理问题，即将中文分词中的串频统计和串匹配结合起来，既发挥匹配分词切分速度快、效率高的特点，有利用了无词典分词结合上下文识 别生词、自动消除歧义的优点。\n\n#### Lucene中的分词器\n\n**1. StopAnalyzerStopAnalyzer**\n能过滤词汇中的特定字符串和词汇，并且完成大写转小写的功能。\n**2. StandardAnalyzer**\n根据空格和符号来完成分词，还可以完成数字、字母、 E-mail 地址、 IP 地址以及中文字符的分析处理，还可以支持过滤词表，用来代替 StopAnalyzer 能够实现的过滤功能。\n**3. SimpleAnalyzer**\n具备基本西文字符词汇分析的分词器，处理词汇单元时，以非字母字符作为分割符号。分词器不能做词汇的过滤，之进行词汇的分析和分割。输出地词汇单元完成小写字符转换，去掉标点符号等分割符。在全文检索系统开发中，通常用来支持西文符号的处理，不支持中文。由于不完成单词过滤功能，所以不需要过滤词库支持。词汇分割策略上简单，使用非英文字符作为分割符，不需要分词词库的支持。\n**4. WhitespaceAnalyzer**\n使用空格作为间隔符的词汇分割分词器。处理词汇单元的时候，以空格字符作为分割符号。分词器不做词汇过滤，也不进行小写字符转换。实际中可以用来支持特定环境下的西文符号的处理。由于不完成单词过滤和小写字符转换功能，也不需要过滤词库支持。词汇分割策略上简单使用非英文字符作为分割符，不需要分词词库支持。\n**5. KeywordAnalyzer**\n把整个输入作为一个单独词汇单元，方便特殊类型的文本进行索引和检索。针对邮政编码，地址等文本信息使用关键词分词器进行索引项建立非常方便\n**6. CJKAnalyzer**\n内部调用 CJKTokenizer 分词器，对中文进行分词，同时使用 StopFilter 过滤器完成过滤功能，可以实现中文的多元切分和停用词过滤。在 Lucene3.0 版本中已经弃用\n**7. ChineseAnalyzer**\n功能与 StandardAnalyzer 分析器在处理中文是基本一致，都是切分成单个的双字节中文字符。在 Lucene3.0 版本中已经弃用。\n**8. PerFieldAnalyzerWrapper**\n功能主要用在针对不同的 Field 采用不同的 Analyzer 的场合。比如对于文件名，需要使用 KeywordAnalyzer ，而对于文件内容只使用 StandardAnalyzer 就可以了。通过 addAnalyzer() 可以添加分类器。\n**9. IKAnalyzer**\n实现了以词典为基础的正反向全切分，以及正反向最大匹配切分两种方法。 IKAnalyzer 是第三方实现的分词器，继承自 Lucene 的 Analyzer 类，针对中文文本进行处理。\n**10. JE-Analysis**\nJE-Analysis 是 Lucene 的中文分词组件，需要下载。\n**11. ICTCLAS4Jictclas4j 中文分词系统**\n是 sinboy 在中科院张华平和刘群老师的研制的 FreeICTCLAS 的基础上完成的一个 java 开源分词项目，简化了原分词程序的复杂度，旨在为广大的中文分词爱好者一个更好的学习机会。\n**12. Imdict-Chinese-Analyzer**\n是 imdict 智能词典 的智能中文分词模块，算法基于隐马尔科夫模型 (Hidden Markov Model, HMM) ，是中国科学院计算技术研究所的 ictclas 中文分词程序的重新实现（基于 Java ），可以直接为 lucene 搜索引擎提供简体中文分词支持。\n**13. Paoding Analysis**\nPaoding Analysis 中文分词具有极 高效率 和 高扩展性 。引入隐喻，采用完全的面向对象设计，构思先进。 其效率比较高，在 PIII 1G 内存个人机器上， 1 秒可准确分词 100 万汉字。 采用基于不限制个数的词典文件对文章进行有效切分，使能够将对词汇分类定义。 能够对未知的词汇进行合理解析。\n**14. MMSeg4J**\nmmseg4j 用 Chih-Hao Tsai 的 MMSeg 算法 实现的中文分词器，并实现 lucene 的 analyzer 和 solr 的 TokenizerFactory 以方便在 Lucene 和 Solr 中使用。 MMSeg 算法有两种分词方法： Simple 和 Complex ，都是基于正向最大匹配。 Complex 加了四个规则过虑。官方说：词语的正确识别率达到了 98.41% 。 mmseg4j 已经实现了这两种分词算法。\n\n\n## Lucene核心内容\n\n### Lucene 组织结构\n \n![](lucene/1.png)\n \n 从图中可以看出，Lucene主要由基础结构封装，索引核心以及对外接口组成，其中索引核心是整个系统的重点。\n \n Lucene的系统设计充分考虑了模块之间的低耦合性，使得其实现更容易理解，也易于扩展。Lucene 一般是作为一个运行库被包含进应用中，而不是以单独的索引服务存在，这也是 Elasticsearch 产生的意义。\n \n### Field（域）的概念\n \n获取原始内容的目的是为了建立索引，在索引前需要将原始内容创建成文档（Document），文档中包括一个一个的域（Field），域（field）在lucene中就是用于搜索和存储分词后的内容，我们在搜索时就是通过域进行的。\n\n例如磁盘中的一个文件我们将它当作一个Document，那么该Document包括这些Field域:\n\n1. file_name：文件名\n2. file_path: 文件路径\n3. file_size: 文件大小\n4. file_content: 文件内容\n \n### 建立和使用索引的过程\n \nLucene 建立索引主要由几个过程组成：\n \n 1. 采集文档\n 2. 建立文档\n 3. 利用分词器分析文档\n 4. 建立倒排索引\n 5. 将索引存入索引库\n\n使用索引的过程：\n\n1. 查询接口\n2. 创建查询对象\n3. 执行查询（从索引库中查询）\n4. 获取查询结果，并将结果返回给用户\n\n![](lucene/2.png)\n\n上图反映了 Lucene 中数据流的流向，图中存在4种数据流：文本流、token流、字节流与查询语句对象流。\n\n* 文本流用以表示将要索引的文件以及向用户输出信息，在Lucene中采用UCS-2编码，以适应多种语言文字处理\n* token流是Lucene内部的概念，是对传统文字中词的概念的抽象，也是Lucene直接处理的最小单位。简单讲就是一个词和所在域值的组合\n* 字节流是对文件抽象的直接操作的体现，通过固定长度的字节流（8 bit）的处理，使文件操作与平台无关\n* 查询语句对象流则是在查询语句解析时使用的概念，它对查询语句抽象，通过类的继承结构反映查询语句的结构，将之传送到查找逻辑进行查找\n\n\n### Lucene 数据类型\n\n在Lucene的web站点上， 有关于Lucene的文件格式的规范，其规定了Lucene的文件格式采取的储存单位、组织结构、命名规范等内容。\n\n在Lucene的文件格式中，以字节为基础，定义了一些数据类型：\n\n1. Byte，占1个字节，基础类型\n2. UInt32，占4个字节， 32位无符号整数，高位优先\n3. UInt64，占8个字节， 64位无符号整数，高位优先\n4. VInt，动态长度整数，最少1个字节，每个字节最高位标识剩余字节数，低七位作为整数的值，高位优先\n5. Chars，动态长度，至少1字节，采用UTF-8编码的Unicode字符序列\n6. String，动态长度，至少2字节，由VInt和Chars组成的字符串类型，VInt表示Chars的长度\n\n以上的数据类型都是以Byte为基础定义的，因此与平台无关。\n\n### Lucene 索引文件结构\n\n![](lucene/3.png)\n\nLucene 索引由若干段（segment）组成，每一段由若干域（field）组成，每个域由若干项（term）组成。\n\n项是最小的索引单位，它代表一个字符串以及其在文件中的位置、出现频次等信息。域是一个关联的元组，由一个域名和一个域值组成，域名是一个字符串，域值是一个项。文档是提取了某个文件中所有信息之后的结果，这些组成了段，或称为一个子索引。子索引可以组合为索引，也可以合并为一个新的子索引。\n\n从结构上来看，索引被处理为一个目录，其中含有的所有文件即为其内容，这些文件按照所属的段不同分组存放，同组的文件拥有相同的文件名，不同的扩展名。此外还有三个文件segments、deletable和lock，分别用于保存所有段的记录、保存已删除文件的记录以及控制读写的同步。\n\n![](lucene/4.png)\n\n每个段的文件中，主要记录了两大类信息：域集合与项集合。域集合与项集合中的文件组采用了一种类似的存储方法：一个小型的索引文件运行时载入内存，一个对应于索引文件的实际信息文件可以按照索引记录的偏移量随机访问。\n\n### Lucene索引构建模块\n\n这部分还在啃......之后更新吧","tags":["Elasticsearch","Lucene"]},{"title":"一次Java函数式编程实践","url":"/2020/10/14/一次Java函数式编程实践/","content":"\n# 一次Java函数式编程实践\n\n## 1. 问题背景\n\n在日常工作中，常会遇到一种场景：在一个方法中，我们需要根据不同的传入参数选择调用不同的方法。\n\n最为简单粗暴的解决方式就是使用 if-else 或者switch 语句去分情况选择\n\n```java\npublic void method(String type){\n    if (type.equals(\"xxx\")) {\n        methodA();\n    }else if (type.equals(\"yyy\")) {\n        methodB();\n    }else if (...) {\n        ...\n    }\n}\n```\n\n但是我个人是比较不喜欢这种写法的，可扩展性太差，不符合开闭原则。想要优化代码，我首先想到的是使用某种设计模式，例如策略模式。\n\n可以定义一个统一的策略接口，编写不同实现类，然后定义 context 对象用于切换上下文。业务代码中只需要将参数传递给 context 对象去选择对应实现类就行。具体可以参考我另一篇博文：[在 Java 中替换大量 if-else 分支的方法](https://fly-bear.github.io/2019/08/15/replace-if/)\n\n不过设计模式虽好，但在开发过程中是比较麻烦的，而且代码量增加了许多，显得有些笨重。很多时候只是一个小功能，不想引入设计模式，有没有更优雅的解决方式呢？\n\n这时候我想到了时下流行的函数式编程。\n\n## 2. 什么是函数式编程\n\n### 简单描述\n\n关于函数式编程的定义和本质，这个问题比较复杂，在这我以自己的理解做一些粗浅的描述。\n\n所谓**函数式编程**（functional programming）指的是一种**编程范式**（programming paradigm），是编程的方法论，例如面向对象编程就是**命令式编程**（Imperative programming）的一种。\n\n命令式编程是面向**计算机硬件**的抽象，有**变量**（对应着存储单元），**赋值语句**（获取，存储指令），**表达式**（内存引用和算术运算）和**控制语句**（跳转指令）。\n\n函数式编程中的**函数**这个术语不是指计算机中的函数（Subroutine），而是指数学中的函数，即自变量的映射是面向数学的抽象，将计算描述为一种**表达式求值**。\n\n不同于命令式编程的将计算过程一步步描述给计算机这种思想，函数式编程倾向于将程序写成各种表达式的集合。\n\n举一个数学上的例子：\n\n$h(g(f(x))) = (h*(g*f))(x) = ((h*g)*f)(x)$\n\n命令式编程就类似于最左边的表达式，一个步骤返回数据后再给另一个步骤执行，而函数式编程就像右边两个表达式，程序表现为函数的交织，最后灌入数据得到结果即可。从这里就能体现出函数式编程的第一个特点：函数是“第一等公民”，和其它类型数据一样，允许作为参数和结果。\n\n### 五个特点：\n\n1. 函数是\"第一等公民\"\n\n2. 只用\"表达式\"，不用\"语句\"。即每一步都是单纯的运算，而且都有返回值。\n\n3. 没有\"副作用\"。意味着函数要保持独立，所有功能就是返回一个新的值，没有其他行为，尤其是不得修改外部变量的值。\n\n4. 不修改状态。意味着状态不能保存在变量中，变量的值是不允许修改的。函数式编程使用参数保存状态，最好的例子就是递归。\n\n5. 引用透明。指的是函数的运行不依赖于外部变量或\"状态\"，只依赖于输入的参数，任何时候只要参数相同，引用函数所得到的返回值总是相同的。\n\n## 3. 代码优化\n\n### Java 8 函数式接口\n\n虽然上面说了这么多，但实际上我并不是想把代码完全改成函数式风格，只想利用它的一个特点——可以将函数作为参数。使用策略模式可以根据参数切换使用的函数，那如果能直接将使用的函数传递进去岂不是更简洁？\n\n其实 Java 早就支持了函数式的编程，最典型的就是 1.8 的 lambda 表达式。除此之外日常编程中接触最多的还有一个，就是 `Stream` 类。`Stream` 中有许多方法可以直接以函数名作为参数，例如 `map()`方法:\n\n```java\npublic final <R> Stream<R> map(Function<? super P_OUT, ? extends R> mapper) {\n    Objects.requireNonNull(mapper);\n    return new StatelessOp<P_OUT, R>(this, StreamShape.REFERENCE, StreamOpFlag.NOT_SORTED | StreamOpFlag.NOT_DISTINCT) {\n        @Override\n        Sink<P_OUT> opWrapSink(int flags, Sink<R> sink) {\n            return new Sink.ChainedReference<P_OUT, R>(sink) {\n                @Override\n                public void accept(P_OUT u) {\n                    downstream.accept(mapper.apply(u));\n                }\n            };\n        }\n    };\n}\n```\n\n`Function`接口在 java.util 中定义，它指定了一个输入类型和一个输入类型（可以使用泛型），简单的方法传递可以直接将参数定义为 Function 类型：\n\n```java\n/**\n* Represents a function that accepts one argument and produces a result.\n*\n* <p>This is a <a href=\"package-summary.html\">functional interface</a>\n* whose functional method is {@link #apply(Object)}.\n*\n* @param <T> the type of the input to the function\n* @param <R> the type of the result of the function\n*\n* @since 1.8\n*/\n\n@FunctionalInterface\npublic interface Function<T, R> {\n\n    R apply(T t);\n\n}\n```\n\n但如果我们的方法有多个传入参数，那么 `Function`就不是很适用了。当然也有可以接受两个入参的 `BiFunction`，不过也并不太泛用。\n\n这时候就需要我们自己实现一个函数式接口。\n\n函数式接口(Functional Interface)是 Java8 引入的新特性，它是有且仅有一个抽象方法，但是可以有多个非抽象方法的接口，它可以被隐式转换为 lambda 表达式。函数式接口可以用`@FunctionalInterface`注解标记，在编译时会进行格式检查。\n\n### 定义接口\n\n假设我有一系列方法：\n\n```java\npublic class MethodClass {\n\n    String methodA(Long l, Integer i, Double b) {\n        ...\n    }\n\n    String methodB(Long l, Integer i, Double b) {\n        ...\n    }\n\n    ...\n}\n```\n\n我需要在另一个方法中按传入条件调用它们中的一个，那么我可以先定义一个函数式接口：\n\n```java\n@FunctionalInterface\n\npublic interface MyFunction {\n\n    String apply(Long l, Integer i, Double d);\n\n}\n```\n\n将接口作为参数类型传递(定义为 static 是为了方便 main 函数调用)，在方法体中调用接口的 apply 方法：\n\n```java\npublic static void method(MyFunction function, Long l, Integer i, Double d) {\n\n    function.apply(l, i, d);\n\n}\n```\n\n于是调用此方法时便可以用 lambda 表达式实现接口后传入：\n\n```java\npublic static void main(String[] args) {\n    MethodClass methodClass = new MethodClass();\n    Long a = 1L;\n    Integer b = 1;\n    Double c = 0D;\n    method((l, i, d) -> methodClass.methodA(l, i, d), a, b, c);\n}\n```\n\n进一步简化可以写成：\n\n```java\npublic static void main(String[] args) {\n    MethodClass methodClass = new MethodClass();\n    Long a = 1L;\n    Integer b = 1;\n    Double c = 0D;\n    method(methodClass::methodA, a, b, c);\n}\n```\n","tags":["Java","函数式编程","代码风格"]},{"title":"使用 Consul+Fabio+Registrator 部署集群","url":"/2020/07/12/Cluster deployment/","content":"\n# 使用 Consul+Fabio+Registrator 部署集群\n\n## 整体结构\n\n### 集群部署\n\n使用 Consul 做服务发现，在中心服务器部署 consul server，各服务器部署 client、 Fabio 和 registrator，将服务自动注册到consul。服务启动时可以向 docker 传递环境变量，设置自己在 consul 的 name 和 tag 等。\n\n使用 Fabio 做负载均衡，从 Consul 注册表读取健康的服务，配置自己的 route 表，根据配置的权重做负载均衡。Fabio 根据 consul tags 的格式可以配置 strip，切分请求 URL，再将流量传递到相应服务。\n\n使用 Fabio 的优点是一个服务调用其它服务时无需关心 consul 集群的位置，只需知道服务名，然后调用本地 4666 端口，由 Fabio 转发到对应服务。另外一个好处是不需要手动配置路由表，更改服务也无需重启 Fabio。\n\n### Node 节点负载均衡\n\n服务部署在 AWS Ec2 云服务上，将其中几台作为 node 节点部署 Fabio 和 Consul server，总流量通过 AWS 提供的弹性负载均衡器（Elastic Load Balance，ELB）分发到不同 node。\n\n### Nginx 反向代理\n\n域名解析指向 node 集群，在 node 上使用 nginx 做反向代理，根据请求的 URL 将流量分发到 Fabio 或者直接到服务，控制访问权限。\n\n## 项目实例\n\n### consul server 部署\n\n单独选择一台或多台稳定的服务器部署 consul server， 例中用了 10.2.26.83\n\n在`/etc/consul_server/`目录下创建配置文件 basic_config.json，写入配置\n\n```\n{\n  \"skip_leave_on_interrupt\": true,\n  \"bind_addr\": \"{{ GetAllInterfaces | include \\\"network\\\" \\\"10.2.0.0/16\\\" | attr \\\"address\\\"}}\",\n  \"client_addr\": \"0.0.0.0\",\n  \"bootstrap_expect\": 2,\n  \"raft_protocol\": 3,\n  \"protocol\": 3,\n  \"retry_join\": [\"provider=aws tag_key=Role tag_value=consul\"],\n  \"ui\": true,\n  \"dns_config\": {\"service_ttl\":\n {\"*\": \"1s\",\n \"redis\": \"300s\"},\n \"enable_truncate\": true,\n \"only_passing\":true}\n}\n```\n\ndocker 以 server 模式启动 consul\n\n```\ndocker run \\\n--name=consul-server \\\n--net=host \\\n-e AWS_ACCESS_KEY_ID=xxxx \\\n-e AWS_SECRET_ACCESS_KEY=yyyy \\\n-e SERVICE_IGNORE=true \\\n-v /etc/flybear/consul_server:/consul/config \\\n--restart=always -d consul:1.0.2 agent -server -bootstrap-expect=1 -node=server-1\n```\n\n其中，\n`-bootstrap-expect=1` 在单节点部署时必须加上，可以选举自己为 leader 节点。如果有多台 server 服务器则可不加\n`-node` 在集群部署时必须，单节点可不加\n\n使用 `docker ps`查看启动情况\n\n![](Cluster%20deployment/1.png)\n\n已经成功启动，使用 `docker exec consul-server consul members`查看集群\n\n![](Cluster%20deployment/2.png)\n\n目前只有一个 server 节点。使用`docker exec consul-server consul info`查看当前节点信息\n\n![](Cluster%20deployment/3.png)\n\n可以看到被选举为 leader 节点\n\n### consul client 部署\n\n登录服务器10.2.26.88，在`/etc/consul_client/`目录下创建配置文件 basic_config.json，写入配置\n\n```\n{\n  \"leave_on_terminate\": true,\n  \"enable_script_checks\": true,\n  \"bind_addr\": \"{{ GetAllInterfaces | include \\\"network\\\" \\\"10.2.0.0/16\\\" | attr \\\"address\\\"}}\",\n  \"client_addr\": \"0.0.0.0\",\n  \"retry_join\": [\"provider=aws tag_key=Role tag_value=consul\"],\n  \"ui\": true,\n  \"raft_protocol\": 3,\n  \"protocol\": 3,\n  \"dns_config\": {\"service_ttl\":\n {\"*\": \"1s\",\n  \"redis\": \"300s\"},\n \"enable_truncate\": true,\n \"only_passing\":true}\n}\n```\n\ndocker 以 client 模式启动 consul\n\n```\ndocker run \\\n--name=consul-client \\\n--net=host \\\n-e AWS_ACCESS_KEY_ID=xxxx \\\n-e AWS_SECRET_ACCESS_KEY=yyyy \\\n-e SERVICE_IGNORE=true \\\n-v /etc/flybear/consul_server:/consul/config \\\n--restart=always -d consul:1.0.2 agent -retry-join=10.2.26.83 -node=client-1\n```\n\n加入之前的 server，`docker ps`查看启动成功后，执行`docker exec consul-client consul members`\n\n![](Cluster%20deployment/4.png)\n\n已经成功加入集群\n\n### Registrator 部署\n\n执行\n\n```\ndocker run \\\n--name=registrator \\\n--net=host \\\n-v /var/run/docker.sock:/tmp/docker.sock \\\n--restart=always -d registrator -ip \"10.2.26.88\" -explicit --deregister-on-stop consul://localhost:8500\n```\n\n`-ip` 指定本机 ip，用于 consul 的健康检查\n\n### Fabio 部署\n\n虽然 fabio 可以零配置，但也可以自定指定一些想要的配置。在`/etc/fabio/`目录下创建配置文件`fabio.properties`，写入\n\n```\nproxy.addr = :4666,:3021;proto=tcp\nproxy.strategy = rr\nregistry.consul.addr = localhost:8500\nregistry.consul.register.enabled = false\nmetrics.statsd.addr = localhost:8125\nlog.level = ERROR\n```\n\n然后使用 docker 启动 Fabio\n\n```\ndocker run \\\n --name=fabio \\\n --net=host \\\n -e SERVICE_9998_NAME=fabio \\\n -e SERVICE_9999_IGNORE=true \\\n -e SERVICE_3021_IGNORE=true \\\n -e SERVICE_4666_IGNORE=true \\\n -v /etc/fabio/fabio.properties:/etc/fabio.properties:rw \\\n --restart=always -d fabiolb/fabio:1.5.2-go1.9 \\\n ./fabio -cfg /etc/fabio.properties\n```\n\n访问集群中任意一台服务的8500端口，查看 consul的可视化页面。\n\n![](Cluster%20deployment/5.png)\n\n可以看到 registrator 已经自动将 fabio 注册到 consul 了\n\n### 服务部署\n\n从 docker 拉取服务镜像，使用 docker 部署\n\n```\ndocker run --init -d --net=host \\\n-v /logs/flybear-api/logs:/logs \\\n-v /etc/flybear/keyfile_dev.json:/etc/keyfile_dev.json \\\n-v /etc/flybear/tl-apns-ent-test.p12:/etc/tl-apns-ent.p12 \\\n-v /home/ubuntu/apollo:/opt/data \\\n-e SERVICE_NAME=flybearapi \\\n-e SERVICE_TAGS='urlprefix-/services/flybearapi strip=/services/flybearapi' \\\n-e SERVICE_CHECK_HTTP='/healthy' \\\n-e SERVICE_CHECK_INTERVAL='15s' \\\n-e SERVICE_CHECK_TIMEOUT='1s' \\\n-e SERVICE_80_CHECK_HTTP=/ \\\n--name=flybearapi \\\n--restart=always flybear/flybear-api-snapshot:0.0.4-SNAPSHOT \\\njava \\\n-server \\\n-XX:+UseG1GC \\\n-Xms512M -Xmx512M \\\n-Xloggc:/logs/gc-`date +%F_%H-%M-%S`.log \\\n-XX:+PrintGCDetails \\\n-XX:+PrintGCDateStamps \\\n-XX:+PrintGCTimeStamps \\\n-XX:+PrintGCCause \\\n-XX:-PrintTenuringDistribution \\\n-XX:+UseGCLogFileRotation \\\n-XX:NumberOfGCLogFiles=5 \\\n-XX:GCLogFileSize=2M \\\n-XX:+HeapDumpOnOutOfMemoryError \\\n-Dcom.sun.management.jmxremote \\\n-Dcom.sun.management.jmxremote.host=127.0.0.1 \\\n-Djava.rmi.server.hostname=127.0.0.1 -Dcom.sun.management.jmxremote.port=9099 \\\n-Dcom.sun.management.jmxremote.authenticate=false \\\n-Dcom.sun.management.jmxremote.ssl=false \\\n-Dspring.boot.admin.client.enabled=true \\\n-Xdebug \\\n-Xrunjdwp:transport=dt_socket,server=y,suspend=n,address=5005 \\\n-jar \\\ntarget/app.jar\n```\n\n指定了注册到 consul 的 name 和 tag 等信息。等待部署完成后到 consul 页面查看\n\n![](Cluster%20deployment/6.png)\n\n健康检查通过，服务成功注册。再访问9998端口查看 fabio 的页面\n\n![](Cluster%20deployment/7.png)\n\nFabio 已经根据 tag 进行了 url 的切分\n\n### Nginx 配置\n\n```nginx\nupstream fabio {\n    server 127.0.0.1:4666 max_fails=0 fail_timeout=1s;\n}\n\nupstream webservers {\n        server 172.31.0.1:8080;\n        server 172.31.0.2:8080;\n        server 172.31.0.3:8080;\n }\n\nserver {\n    listen 80;\n\n    gzip on;\n    gzip_types      application/json;\n    gzip_proxied    no-cache no-store private expired auth;\n    gzip_min_length 1000;\n    server_name flybear.com;\n    access_log /var/log/nginx/host/access.log combined_plus;\n    error_log  /var/log/nginx/host/error.log error;\n\n    location /internal {\n        return 404;\n        log_not_found off;\n    }\n\n    location / {\n        proxy_pass http://fabio;\n    }\n\n    location = /test {\n        proxy_pass http://webservers;\n    }\n}\n```\n\n可以控制内部接口不允许公开访问，将一些特殊地址转向特定服务器等","tags":["集群部署","服务架构"]},{"title":"数据库调优","url":"/2020/01/10/数据库调优/","content":"## 慢查询\n\n### 处理步骤\n\n1. 判断慢查询产生（CPU负载、IO读写、执行时间）\n2. 打开慢查询日志或使用分析工具（mysqldumpslow等）\n3. 选择调优方式\n\n### 性能调优\n\n#### 应用程序优化\n1. 减少数据库连接次数，空间换时间\n2. 拆分复杂语句，多表分别查询\n\n#### SQL语句优化\n\n1. 避免使用 SELECT *\n2. 避免负向查询（NOT != <> !< !> MOT IN NOT LIKE）和%开头的like（前导模糊查询）--会导致全表扫描\n3. 避免大表使用JOIN查询和子查询--会产生临时表，消耗较多CPU和内存，影响数据库性能\n4. 确定只有一条记录返回，可以加上limit 1\n5. 可以使用 exist 和 not exist 代替 in 和 not in\n6. WHERE 语句中对字段做计算操作、使用函数、类型转换等会导致无法命中索引\n\n#### 表结构优化\n\n1. 字段类型优化，使用合适的类型（字段长度，避免 text，使用 not null）\n2. 合理使用索引，去除无用索引\n3. 读写分离和分库分表\n4. 避免使用触发器，存储过程、外键等\n\n#### 硬件和数据库配置优化\n1. 集群和分布式部署，减少单台机器压力\n2. 升级机器配置\n3. 使用合适的储存引擎，表锁、行锁的选择\n4. 增加缓存系统\n\n### 全文索引\n\n#### MySQL\n\n##### 版本支持\n\n1. MySQL 5.6 以前的版本，只有 MyISAM 存储引擎支持全文索引；\n2. MySQL 5.6 及以后的版本，MyISAM 和 InnoDB 存储引擎均支持全文索引;\n3. 只有字段的数据类型为 char、varchar、text 及其系列才可以建全文索引。\n\n##### 创建\n\n* 建表时创建：\n```\ncreate table TABLE_NAME(\n    id int NOT NULL AUTO_INCREMENT,\n    content text NOT NULL,\n    name varchar(255),\n    PRIMARY KEY (id),\n    FULLTEXT KEY content_name_fulltext(content,name)  // 创建联合全文索引列\n) ENGINE=MyISAM DEFAULT CHARSET=utf8;\n```\n\n* 已存在的表上创建\n```\ncreate fulltext index content_name_fulltext on fulltext_test(content,name);\n```\n或\n```\nalter table fulltext_test add fulltext index content_name_fulltext(content,name);\n```\n\n* 删除\n```\ndrop index content_name_fulltext on fulltext_test;\n```\n或\n```\nalter table fulltext_test drop index content_name_fulltext;\n```\n\n### explain 语句的应用\n\n使用 explain 可以得到以下信息\n* 表的读取顺序\n* 数据读取操作的类型\n* 哪些索引可以使用\n* 哪些索引实际被使用\n* 表之间的引用\n* 每张表有多少行被优化器扫描\n\n#### id\nSQL执行的顺序的标识,SQL从大到小的执行\n\n1. id相同时，执行顺序由上至下\n2. 如果是子查询，id的序号会递增，id值越大优先级越高，越先被执行\n3. id如果相同，可以认为是一组，从上往下顺序执行；在所有组中，id值越大，优先级越高，越先执行 \n \n#### select_type\n查询中每个select子句的类型\n1. SIMPLE(简单SELECT,不使用UNION或子查询等)\n2. PRIMARY(查询中若包含任何复杂的子部分,最外层的select被标记为PRIMARY)\n3. UNION(UNION中的第二个或后面的SELECT语句)\n4. DEPENDENT UNION(UNION中的第二个或后面的SELECT语句，取决于外面的查询)\n5. UNION RESULT(UNION的结果)\n6. SUBQUERY(子查询中的第一个SELECT)\n7. DEPENDENT SUBQUERY(子查询中的第一个SELECT，取决于外面的查询)\n8. DERIVED(派生表的SELECT, FROM子句的子查询)\n9. UNCACHEABLE SUBQUERY(一个子查询的结果不能被缓存，必须重新评估外链接的第一行)\n \n#### table\n显示这一行的数据是关于哪张表的，有时不是真实的表名字,看到的是derivedx(x是个数字,我的理解是第几步执行的结果)\n\n#### type\n表示MySQL在表中找到所需行的方式，又称“访问类型”。\n常用的类型有： ALL, index,  range, ref, eq_ref, const, system, NULL（从左到右，性能从差到好）\n\n \n#### possible_keys\n指出MySQL能使用哪个索引在表中找到记录\n \n#### Key\n显示MySQL实际决定使用的键（索引）如果没有选择索引，键是NULL。要想强制MySQL使用或忽视possible_keys列中的索引，在查询中使用FORCE INDEX、USE INDEX或者IGNORE INDEX。\n \n#### key_len\n表示索引中使用的字节数，可通过该列计算查询中使用的索引的长度。不损失精确性的情况下，长度越短越好 \n \n#### ref\n表示上述表的连接匹配条件，即哪些列或常量被用于查找索引列上的值 \n\n#### rows \n表示MySQL根据表统计信息及索引选用情况，估算的找到所需的记录所需要读取的行数\n \n#### Extra\n该列包含MySQL解决查询的详细信息\n \n \n#### 总结：\n* EXPLAIN不会告诉你关于触发器、存储过程的信息或用户自定义函数对查询的影响情况\n* EXPLAIN不考虑各种Cache\n* EXPLAIN不能显示MySQL在执行查询时所作的优化工作\n* 部分统计信息是估算的，并非精确值\n* EXPALIN只能解释SELECT操作，其他操作要重写为SELECT后查看执行计划","tags":["数据库","MySQL"]},{"title":"日志系统搭建","url":"/2019/09/08/database/","content":"# 日志系统搭建\n\n### 产生日志\n\n搭建 **logback + slf4j** 框架，定义日志文件路径和级别，在项目中输出日志\n\n### 日志管理\n\n##### **收集日志**\n\n* 每台服务器启动 filebeat 服务，读取日志文件，正则匹配，统一发送到 logstash\n* logstash 服务器收集所有日志保存到统一路径下管理\n* fluentd 读取日志文件，解析成 Elasticsearch 所用格式，调用 ES 接口导入\n\n##### **日志显示**\n\n部署 Kibana 连接 ES，建立日常所用索引\n\n### filebeat.yaml 配置\n\n```\nfilebeat.inputs:\n- type: log\n  paths:\n    - /home/ubuntu/logs/app.log\n  fields:\n    type: app\n    business: my-project\n  fields_under_root: true\n  multiline.pattern: '^\\[[0-9]{4}-[0-9]{2}-[0-9]{2}'\n  multiline.negate: true\n  multiline.match: after\n  multiline.max_lines: 10000\n  multiline.timeout: 20\n\n- type: log\n  paths:\n    - /home/ubuntu/logs/access/access.log\n  fields:\n      type: access\n      business: my-project\n  fields_under_root: true\n  \n#---------------------- Logstash output --------------------------\noutput.logstash:\n  hosts: [\"logstash-prod.flybear.com:5044\"]\n```\n\n#### 多行合并\n当 log 输出多行时，kibana 上显示为多条，需要进行合并。日志以日期格式 yyyy-MM-dd 开头，配置 filebeat.yml 添加\n ```\n multiline.pattern: '^\\[[0-9]{4}-[0-9]{2}-[0-9]{2}'\n multiline.negate: true\n multiline.match: after\n multiline.max_lines: 10000\n multiline.timeout: 20\n```\n**pattern**：正则表达式\n\n**negate**：true 或 false；默认是false，匹配pattern的行合并到上一行；true，不匹配pattern的行合并到上一行\n\n**match**：after 或 before，合并到上一行的末尾或开头\n\n### logstash.conf 配置\n```\n input {\n  beats {\n    port => 5044\n  }\n}\n\nfilter {\n   ruby {\n       code => \"\n            event.timestamp.time.localtime\n            tstamp = event.get('@timestamp').to_i\n            event.set('date_str', Time.at(tstamp).strftime('%Y-%m-%d'))\n        \"\n   }\n}\n\noutput {\n  file {\n    path => \"/var/log/remote/logstash/%{business}-%{type}-%{[agent][hostname]}.%{date_str}.log\"\n    codec => line { format => \"%{message}\" }\n  }\n}\n```\n\n###  fluentd.conf 配置\n\n#### 配置日志源\n```\n<source>\n  @type tail\n  @id mongodb-prod1\n  path /logs/logstash/my-project-app-*.log\n  pos_file /tmp/my-project.pos\n  refresh_interval 30\n  tag mongodb-prod1\n  path_key log_path\n  <parse>\n    @type multiline\n    format_firstline /^\\d{4}-\\d{2}-\\d{2}T\\d{2}:\\d{2}:\\d{2}/\n    format1 /^(?<mongotime>[^ ]*)[ ]*(?<level>[^ ]*)[ ]*(?<command>[^ ]*)[ ]*\\[(?<content>[^\\]]*)\\][ ]*(?<msg>[^$]*)/\n    time_format %Y-%m-%d-%H:%M:%S\n  </parse>\n</source>\n```\n可以为不同的日志文件配置各自的解析格式\n\n#### 配置输出\n\n```\n<match my-project my-project2>\n@type elasticsearch\n@id elasticsearch_output\n  host 172.31.26.211\n  port 9200\n  logstash_prefix ${tag}\n  logstash_format true\n  slow_flush_log_threshold 40.0\n  <buffer>\n    @type file\n    path /fluentd/log/elasticsearch.log\n    flush_mode interval\n    retry_type exponential_backoff\n    flush_thread_count 32\n    flush_interval 2s\n    retry_forever false\n    retry_max_interval 60\n    chunk_limit_size 5M\n    queue_limit_length 2000\n    overflow_action throw_exception\n  </buffer>\n</match>\n```\n指定所需的所有源，配置输出到 es 的 ip, port，指定缓存路径，分批导入 ES。\n\n","tags":["ELK","Filebeat","Fluentd"]},{"title":"在 Java 中替换大量 if-else 分支的方法","url":"/2019/08/15/replace-if/","content":"## 问题描述\n 在日常的开发中，有时会写出一些功能正常但不够简洁优雅的代码，例如，滥用 if-else。\n\n### 案例一\n在某个业务场景中，需要联合多个数据源查询数据，因为同时涉及到关系型和非关系型数据库，无法通过单一的 SQL 实现，只能分别去查。\n\n于是，问题就出现了。由于是用前一个数据库查到的数据作为条件去另一个数据库查询，在执行之前就需要先判断一下是否为 null（为了使代码显得优雅，这里可以使用` Optional` 类代替 `xx == null`这样的语句。），结果就是出现了令人厌恶的多重嵌套分支：\n\n```\nif (!list.isEmpty()) {\n    ...\n    if (optional.isPresent()) {\n        ...\n        if (field != null) {\n            ...\n            if (...) {\n                ...\n            }\n        }\n    }\n}\n```\n\n### 案例二\n 有时也经常遇到这样的情况：要根据不同条件执行不同的操作，例如根据某变量的取值决定执行的代码：\n\n```\nif (a == xxx) {\n    ...\n}else if {\n    ...\n}else if {\n    ...\n}else if {\n...\n```\n\n 当然，我们完全可以使用 switch case 结构来实现这样的功能，但是 switch 语句过长仍然会导致可读性和扩展性差，违背程序设计的开闭原则。甚至，在业务变更的时候，switch case 比 if else 更加难以进行修改。\n\n## 优化思路\n 很显然，这样的代码实在过于丑陋。\n\n 正好，我目前在读阿里巴巴的孤尽大神所著的《码出高效：Java 开发手册》这本书，里面有一段就是描述了这种情况：\n\n> 多重嵌套不能超过三层。多层嵌套在哪里都不受欢迎，是因为条件判断和分支逻辑数量呈指数关系。如果非得使用多层嵌套，请使用状态设计模式。对于超过3层的 if-else 的逻辑判断代码，可以使用卫语句、策略模式、状态模式等来实现\n\n 既然大神给提供了思路，那就试着去做呗！根据我脑子里那点可怜的 Java 基础，艰难地归纳了一下这几种方案的具体实现。\n\n## 方案实现\n### 1. 卫语句\n 所谓 **卫语句(guard clauses)** 就是把复杂的条件表达式拆分成多个条件表达式。例如在案例一中，很多的判断其实只是为了确定是否需要执行下一步，那么就可以在判断不满足条件后直接 return。修改后的结构如下：\n\n```\nif (list.isEmpty()) {\n    return null;\n}\n...\nif (!optional.isPresent()) {\n    return null;\n}\n...\nif (field == null) {\n    return null;\n}\n...\nif (...) {\n    ...\n}\n```\n\n### 2. 策略模式\n**策略模式（Strategy Pattern）** 属于行为型的设计模式，其用意是针对一组算法，将每一个算法封装到具有共同接口的独立的类中，从而使得它们可以相互替换。策略模式使得算法可以在不影响到客户端的情况下发生变化。\n\n 例如 switch 语句的每个不同 case 分支可以看作是执行不同策略，那么将这些策略抽取出来，封装到类中，然后定义一个公共的接口以供调用，就可以用一段调用代码替换掉整个分支结构。\n\n 要使用策略模式，首先就是要定义调用的接口：\n\n```\npublic interface Strategy {\n    public void execute();\n}\n```\n\n 然后，定义具体策略实现类：\n\n```\npublic class StrategyA implements Strategy {\n\n    @override\n    public void execute() {\n        ... //do something\n    }\n}\n\npublic class StrategyB implements Strategy {\n    @override\n    public void execute() {\n        ... //do something\n    }\n}\npublic class StrategyC implements Strategy {\n    @override\n    public void execute() {\n        ... //do something\n    }\n}\n```\n\n\n最后，定义 context对象以调用实际策略：\n\n```\npublic class Context {\n\n    Strategy strategy;\n\n    public Context(Strategy strategy) {\n        this.strategy = strategy;\n    }\n    \n    public execute() {\n        this.strategy.execute();\n    }\n}\n```\n\n 这里注意，需要传入对应的实例对象以调用它的方法，一般其它教程会直接创建或者使用简单工厂模式以获得对象，例如：\n\n```\npublic class contextFactory {\n\n    public getStrategy(String strategyStr) {\n        switch (strategyStr) {\n            case \"A\" : return new StrategyA();\n            case \"B\" : return new StrategyB();\n            case \"C\" : return new StrategyC();\n            ...\n        }\n    }\n}\n```\n\n 发现没有？这样又重新引入了 if 或者 switch，这就与我们的初衷相违背了。那么，该如何避免呢？于是我想到了反射（reflect）。使用反射，我们可以直接通过策略名称获得相应的对象，修改工厂类代码如下：\n\n```\npublic class ContextFactory {\n\n    public getStrategy(String strategyStr) {\n        return (Strategy) Class.forName(\"Strategy\" + strategyStr); //根据类名返回对应实例\n    }\n}\n```\n\n 在实际代码中使用：\n\n```\nString strategyStr;\nContextFactory contextFactory = new ContextFactory();\n...\ntry {\t//注意加 try catch 块\n    Strategy strategy = contextFactory.getStrategy(strategyStr); //获取策略对象\n    strategy.execute(); //执行具体操作\n}catch (Exception e) {\n    e.printStackTrace();\n}\n...\n```\n\n### 3. 状态模式\n待补充","tags":["Java","代码风格"]},{"title":"解决 jpa 的 save 过程用 null 值覆盖数据库有效值的问题","url":"/2019/07/19/解决-jpa-的-save-过程用-null-值覆盖数据库有效值的问题/","content":"\n## 问题背景\n\n最近在项目中使用 jpa 操作数据库的时候，遇到了一个问题。\n\n我所使用的数据库是 postgreSQL，通过定义实体类添加`@Table`注解指定表，定义接口继承`CrudRepository`类，将实体类作为参数传入，即可调用默认的 save 方法，jpa 会根据主键将实体进行插入或更新操作。\n\n但是，如果用于更新的实体中含有未赋值的属性，即值为 null 的情况时，数据库中对应字段的值会被 null 覆盖。\n\n我先是在 Google 查询了解决方案，有网友提出可以在实体类加上`@DynamicUpdate`和`@DynamicInsert`两个注解，序列化时即可忽略为 null 的值。我尝试了这种方法，启动单元测试进行了一次更新操作，却发现数据库的数据依然被 null 覆盖。\n\n为了寻找原因，我查看了这两个注解的源码，它们是在 hebernate-core 里定义的。使用 idea 的全局搜索找到了注解的实现代码,还没来得及看具体实现，却发现它被加上了`@Deprecated`注解。虽然理论上对使用是无影响的，但既然作者不推荐了，还是放弃使用吧。\n\n## 自己动手\n实在不想在苦苦寻找方法了，决定自己造个轮子，实现 update 的功能。逻辑非常简单，每次 save 之前先从数据库查找出对应数据存入实体A，将含有更新数据的实体B中的非空属性赋值给实体A，最后用实体 A 存回数据库。具体代码如下：\n\n```\n\t/**\n     * 防止 jpa 将为 null 的属性更新（因为注解DynamicUpdate无效）\n     *\n     * @param entity 包含需要更新字段的实体\n     * @param oldEntity 从数据库获取的旧实体\n     * @return 最终往数据库插入的实体\n     */\n     \n    public static <T> T updateEntity(T entity,T oldEntity){\n        Class clazz = entity.getClass();\n        Field[] fields = clazz.getDeclaredFields();\n        for (Field field : fields){\n            try {\n                PropertyDescriptor pd = new PropertyDescriptor(field.getName(), clazz);\n                Method readMethod = pd.getReadMethod();\n                Method writeMethod= pd.getWriteMethod();\n                if (readMethod.invoke(entity)!=null){   //不为null 的覆盖\n                    writeMethod.invoke(oldEntity,readMethod.invoke(entity));\n                }\n            }catch (Exception e){\n                log.error(e.getMessage());\n            }\n        }\n        return oldEntity;\n    }\n```\n\n利用反射+泛型写了个通用方法，将新旧实体作为参数传入，反射取出新实体中非 null 的值赋给旧实体，最后将旧实体返回。将方法封装成工具类方便调用。\n\n在代码中，每次更新数据库前调用一下工具，用返回的实体作为 save 对象，经测试，成功解决 null 覆盖问题。\n\n## 现成工具类\n\n实际上，通过寻找，我确实发现了现成的工具。\n\n大家都知道 `BeanUtil` 类的 `copyProperties` 方法可以拷贝两个实体的同名变量，而通过控制传入参数，就可以实现只拷贝非空属性。\n\n`BeanUtil` 中有个 `getNullPropertyNames` 方法：\n\n```\n\tpublic static String[] getNullPropertyNames (Object source) {\n\t\tfinal BeanWrapper src = new BeanWrapperImpl(source);\n\t\tjava.beans.PropertyDescriptor[] pds = src.getPropertyDescriptors();\n\n\t\tSet<String> emptyNames = new HashSet<String>();\n\t\tfor(java.beans.PropertyDescriptor pd : pds) {\n\t\t\tObject srcValue = src.getPropertyValue(pd.getName());\n\t\t\tif (srcValue == null) emptyNames.add(pd.getName());\n\t\t}\n\t\tString[] result = new String[emptyNames.size()];\n\t\treturn emptyNames.toArray(result);\n\t}\n```\n\n此方法返回实体的所有非 null 字段，将其作为参数传入 `copyProperties` ，即可实现指定复制。代码如下：\n\n```\n\tpublic void update(ClassA source, Integer id ){\n\t\tClassA target = classDao.findById(id);  \n\t\tBeanUtils.copyProperties(source,target,getNullPropertyNames(source)); \n\t\tclassDao.update(target );\n}\n```","tags":["Java","jpa","postgreSQL"]}]