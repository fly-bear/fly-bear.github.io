[{"title":"数据库调优","url":"/2020/01/10/数据库调优/","content":"## 慢查询\n\n### 处理步骤\n\n1. 判断慢查询产生（CPU负载、IO读写、执行时间）\n2. 打开慢查询日志或使用分析工具（mysqldumpslow等）\n3. 选择调优方式\n\n### 性能调优\n\n#### 应用程序优化\n1. 减少数据库连接次数，空间换时间\n2. 拆分复杂语句，多表分别查询\n\n#### SQL语句优化\n\n1. 避免使用 SELECT *\n2. 避免负向查询（NOT != <> !< !> MOT IN NOT LIKE）和%开头的like（前导模糊查询）--会导致全表扫描\n3. 避免大表使用JOIN查询和子查询--会产生临时表，消耗较多CPU和内存，影响数据库性能\n4. 确定只有一条记录返回，可以加上limit 1\n5. 可以使用 exist 和 not exist 代替 in 和 not in\n6. WHERE 语句中对字段做计算操作、使用函数、类型转换等会导致无法命中索引\n\n#### 表结构优化\n\n1. 字段类型优化，使用合适的类型（字段长度，避免 text，使用 not null）\n2. 合理使用索引，去除无用索引\n3. 读写分离和分库分表\n4. 避免使用触发器，存储过程、外键等\n\n#### 硬件和数据库配置优化\n1. 集群和分布式部署，减少单台机器压力\n2. 升级机器配置\n3. 使用合适的储存引擎，表锁、行锁的选择\n4. 增加缓存系统\n\n### 全文索引\n\n#### MySQL\n\n##### 版本支持\n\n1. MySQL 5.6 以前的版本，只有 MyISAM 存储引擎支持全文索引；\n2. MySQL 5.6 及以后的版本，MyISAM 和 InnoDB 存储引擎均支持全文索引;\n3. 只有字段的数据类型为 char、varchar、text 及其系列才可以建全文索引。\n\n##### 创建\n\n* 建表时创建：\n```\ncreate table TABLE_NAME(\n    id int NOT NULL AUTO_INCREMENT,\n    content text NOT NULL,\n    name varchar(255),\n    PRIMARY KEY (id),\n    FULLTEXT KEY content_name_fulltext(content,name)  // 创建联合全文索引列\n) ENGINE=MyISAM DEFAULT CHARSET=utf8;\n```\n\n* 已存在的表上创建\n```\ncreate fulltext index content_name_fulltext on fulltext_test(content,name);\n```\n或\n```\nalter table fulltext_test add fulltext index content_name_fulltext(content,name);\n```\n\n* 删除\n```\ndrop index content_name_fulltext on fulltext_test;\n```\n或\n```\nalter table fulltext_test drop index content_name_fulltext;\n```\n\n### explain 语句的应用\n\n使用 explain 可以得到以下信息\n* 表的读取顺序\n* 数据读取操作的类型\n* 哪些索引可以使用\n* 哪些索引实际被使用\n* 表之间的引用\n* 每张表有多少行被优化器扫描\n\n#### id\nSQL执行的顺序的标识,SQL从大到小的执行\n\n1. id相同时，执行顺序由上至下\n2. 如果是子查询，id的序号会递增，id值越大优先级越高，越先被执行\n3. id如果相同，可以认为是一组，从上往下顺序执行；在所有组中，id值越大，优先级越高，越先执行 \n \n#### select_type\n查询中每个select子句的类型\n1. SIMPLE(简单SELECT,不使用UNION或子查询等)\n2. PRIMARY(查询中若包含任何复杂的子部分,最外层的select被标记为PRIMARY)\n3. UNION(UNION中的第二个或后面的SELECT语句)\n4. DEPENDENT UNION(UNION中的第二个或后面的SELECT语句，取决于外面的查询)\n5. UNION RESULT(UNION的结果)\n6. SUBQUERY(子查询中的第一个SELECT)\n7. DEPENDENT SUBQUERY(子查询中的第一个SELECT，取决于外面的查询)\n8. DERIVED(派生表的SELECT, FROM子句的子查询)\n9. UNCACHEABLE SUBQUERY(一个子查询的结果不能被缓存，必须重新评估外链接的第一行)\n \n#### table\n显示这一行的数据是关于哪张表的，有时不是真实的表名字,看到的是derivedx(x是个数字,我的理解是第几步执行的结果)\n\n#### type\n表示MySQL在表中找到所需行的方式，又称“访问类型”。\n常用的类型有： ALL, index,  range, ref, eq_ref, const, system, NULL（从左到右，性能从差到好）\n\n \n#### possible_keys\n指出MySQL能使用哪个索引在表中找到记录\n \n#### Key\n显示MySQL实际决定使用的键（索引）如果没有选择索引，键是NULL。要想强制MySQL使用或忽视possible_keys列中的索引，在查询中使用FORCE INDEX、USE INDEX或者IGNORE INDEX。\n \n#### key_len\n表示索引中使用的字节数，可通过该列计算查询中使用的索引的长度。不损失精确性的情况下，长度越短越好 \n \n#### ref\n表示上述表的连接匹配条件，即哪些列或常量被用于查找索引列上的值 \n\n#### rows \n表示MySQL根据表统计信息及索引选用情况，估算的找到所需的记录所需要读取的行数\n \n#### Extra\n该列包含MySQL解决查询的详细信息\n \n \n#### 总结：\n* EXPLAIN不会告诉你关于触发器、存储过程的信息或用户自定义函数对查询的影响情况\n* EXPLAIN不考虑各种Cache\n* EXPLAIN不能显示MySQL在执行查询时所作的优化工作\n* 部分统计信息是估算的，并非精确值\n* EXPALIN只能解释SELECT操作，其他操作要重写为SELECT后查看执行计划","tags":["数据库","MySQL"]},{"title":"日志系统搭建","url":"/2019/09/08/database/","content":"# 日志系统搭建\n\n### 产生日志\n\n搭建 **logback + slf4j** 框架，定义日志文件路径和级别，在项目中输出日志\n\n### 日志管理\n\n##### **收集日志**\n\n* 每台服务器启动 filebeat 服务，读取日志文件，正则匹配，统一发送到 logstash\n* logstash 服务器收集所有日志保存到统一路径下管理\n* fluentd 读取日志文件，解析成 Elasticsearch 所用格式，调用 ES 接口导入\n\n##### **日志显示**\n\n部署 Kibana 连接 ES，建立日常所用索引\n\n### filebeat.yaml 配置\n\n```\nfilebeat.inputs:\n- type: log\n  paths:\n    - /home/ubuntu/logs/app.log\n  fields:\n    type: app\n    business: backoffice-api\n  fields_under_root: true\n  multiline.pattern: '^\\[[0-9]{4}-[0-9]{2}-[0-9]{2}'\n  multiline.negate: true\n  multiline.match: after\n  multiline.max_lines: 10000\n  multiline.timeout: 20\n\n- type: log\n  paths:\n    - /home/ubuntu/logs/access/access.log\n  fields:\n      type: access\n      business: backoffice-api\n  fields_under_root: true\n  \n#---------------------- Logstash output --------------------------\noutput.logstash:\n  hosts: [\"logstash-prod.truckerpath.com:5044\"]\n```\n\n#### 多行合并\n当 log 输出多行时，kibana 上显示为多条，需要进行合并。日志以日期格式 yyyy-MM-dd 开头，配置 filebeat.yml 添加\n ```\n multiline.pattern: '^\\[[0-9]{4}-[0-9]{2}-[0-9]{2}'\n multiline.negate: true\n multiline.match: after\n multiline.max_lines: 10000\n multiline.timeout: 20\n```\n**pattern**：正则表达式\n\n**negate**：true 或 false；默认是false，匹配pattern的行合并到上一行；true，不匹配pattern的行合并到上一行\n\n**match**：after 或 before，合并到上一行的末尾或开头\n\n### logstash.conf 配置\n```\n input {\n  beats {\n    port => 5044\n  }\n}\n\nfilter {\n   ruby {\n       code => \"\n            event.timestamp.time.localtime\n            tstamp = event.get('@timestamp').to_i\n            event.set('date_str', Time.at(tstamp).strftime('%Y-%m-%d'))\n        \"\n   }\n}\n\noutput {\n  file {\n    path => \"/var/log/remote/logstash/%{business}-%{type}-%{[agent][hostname]}.%{date_str}.log\"\n    codec => line { format => \"%{message}\" }\n  }\n}\n```\n\n###  fluentd.conf 配置\n\n#### 配置日志源\n```\n<source>\n  @type tail\n  @id mongodb-prod1\n  path /logs/logstash/mongodb-prod1-app-*.log\n  pos_file /tmp/mongodb-prod1.pos\n  refresh_interval 30\n  tag mongodb-prod1\n  path_key log_path\n  <parse>\n    @type multiline\n    format_firstline /^\\d{4}-\\d{2}-\\d{2}T\\d{2}:\\d{2}:\\d{2}/\n    format1 /^(?<mongotime>[^ ]*)[ ]*(?<level>[^ ]*)[ ]*(?<command>[^ ]*)[ ]*\\[(?<content>[^\\]]*)\\][ ]*(?<msg>[^$]*)/\n    time_format %Y-%m-%d-%H:%M:%S\n  </parse>\n</source>\n```\n可以为不同的日志文件配置各自的解析格式\n\n#### 配置输出\n\n```\n<match nginx_access_error nginx_access_normal tpnewapi_app backoffice_access backoffice_app job_executor_app  tpnewapi_data.** job_executor_data.**>\n@type elasticsearch\n@id elasticsearch_output\n  host 172.31.26.211\n  port 9200\n  logstash_prefix ${tag}\n  logstash_format true\n  slow_flush_log_threshold 40.0\n  <buffer>\n    @type file\n    path /fluentd/log/elasticsearch.log\n    flush_mode interval\n    retry_type exponential_backoff\n    flush_thread_count 32\n    flush_interval 2s\n    retry_forever false\n    retry_max_interval 60\n    chunk_limit_size 5M\n    queue_limit_length 2000\n    overflow_action throw_exception\n  </buffer>\n</match>\n```\n指定所需的所有源，配置输出到 es 的 ip, port，指定缓存路径，分批导入 ES。\n\n"},{"title":"在 Java 中替换大量 if-else 分支的方法","url":"/2019/08/15/replace-if/","content":"## 问题描述\n 在日常的开发中，有时会写出一些功能正常但不够简洁优雅的代码，例如，滥用 if-else。\n\n### 案例一\n在某个业务场景中，需要联合多个数据源查询数据，因为同时涉及到关系型和非关系型数据库，无法通过单一的 SQL 实现，只能分别去查。\n\n于是，问题就出现了。由于是用前一个数据库查到的数据作为条件去另一个数据库查询，在执行之前就需要先判断一下是否为 null（为了使代码显得优雅，这里可以使用` Optional` 类代替 `xx == null`这样的语句。），结果就是出现了令人厌恶的多重嵌套分支：\n\n```\nif (!list.isEmpty()) {\n    ...\n    if (optional.isPresent()) {\n        ...\n        if (field != null) {\n            ...\n            if (...) {\n                ...\n            }\n        }\n    }\n}\n```\n\n### 案例二\n 有时也经常遇到这样的情况：要根据不同条件执行不同的操作，例如根据某变量的取值决定执行的代码：\n\n```\nif (a == xxx) {\n    ...\n}else if {\n    ...\n}else if {\n    ...\n}else if {\n...\n```\n\n 当然，我们完全可以使用 switch case 结构来实现这样的功能，但是 switch 语句过长仍然会导致可读性和扩展性差，违背程序设计的开闭原则。甚至，在业务变更的时候，switch case 比 if else 更加难以进行修改。\n\n## 优化思路\n 很显然，这样的代码实在过于丑陋。\n\n 正好，我目前在读阿里巴巴的孤尽大神所著的《码出高效：Java 开发手册》这本书，里面有一段就是描述了这种情况：\n\n> 多重嵌套不能超过三层。多层嵌套在哪里都不受欢迎，是因为条件判断和分支逻辑数量呈指数关系。如果非得使用多层嵌套，请使用状态设计模式。对于超过3层的 if-else 的逻辑判断代码，可以使用卫语句、策略模式、状态模式等来实现\n\n 既然大神给提供了思路，那就试着去做呗！根据我脑子里那点可怜的 Java 基础，艰难地归纳了一下这几种方案的具体实现。\n\n## 方案实现\n### 1. 卫语句\n 所谓 **卫语句(guard clauses)** 就是把复杂的条件表达式拆分成多个条件表达式。例如在案例一中，很多的判断其实只是为了确定是否需要执行下一步，那么就可以在判断不满足条件后直接 return。修改后的结构如下：\n\n```\nif (list.isEmpty()) {\n    return null;\n}\n...\nif (!optional.isPresent()) {\n    return null;\n}\n...\nif (field == null) {\n    return null;\n}\n...\nif (...) {\n    ...\n}\n```\n\n### 2. 策略模式\n**策略模式（Strategy Pattern）** 属于行为型的设计模式，其用意是针对一组算法，将每一个算法封装到具有共同接口的独立的类中，从而使得它们可以相互替换。策略模式使得算法可以在不影响到客户端的情况下发生变化。\n\n 例如 switch 语句的每个不同 case 分支可以看作是执行不同策略，那么将这些策略抽取出来，封装到类中，然后定义一个公共的接口以供调用，就可以用一段调用代码替换掉整个分支结构。\n\n 要使用策略模式，首先就是要定义调用的接口：\n\n```\npublic interface Strategy {\n    public void execute();\n}\n```\n\n 然后，定义具体策略实现类：\n\n```\npublic class StrategyA implements Strategy {\n\n    @override\n    public void execute() {\n        ... //do something\n    }\n}\n\npublic class StrategyB implements Strategy {\n    @override\n    public void execute() {\n        ... //do something\n    }\n}\npublic class StrategyC implements Strategy {\n    @override\n    public void execute() {\n        ... //do something\n    }\n}\n```\n\n\n最后，定义 context对象以调用实际策略：\n\n```\npublic class Context {\n\n    Strategy strategy;\n\n    public Context(Strategy strategy) {\n        this.strategy = strategy;\n    }\n    \n    public execute() {\n        this.strategy.execute();\n    }\n}\n```\n\n 这里注意，需要传入对应的实例对象以调用它的方法，一般其它教程会直接创建或者使用简单工厂模式以获得对象，例如：\n\n```\npublic class contextFactory {\n\n    public getStrategy(String strategyStr) {\n        switch (strategyStr) {\n            case \"A\" : return new StrategyA();\n            case \"B\" : return new StrategyB();\n            case \"C\" : return new StrategyC();\n            ...\n        }\n    }\n}\n```\n\n 发现没有？这样又重新引入了 if 或者 switch，这就与我们的初衷相违背了。那么，该如何避免呢？于是我想到了反射（reflect）。使用反射，我们可以直接通过策略名称获得相应的对象，修改工厂类代码如下：\n\n```\npublic class ContextFactory {\n\n    public getStrategy(String strategyStr) {\n        return (Strategy) Class.forName(\"Strategy\" + strategyStr); //根据类名返回对应实例\n    }\n}\n```\n\n 在实际代码中使用：\n\n```\nString strategyStr;\nContextFactory contextFactory = new ContextFactory();\n...\ntry {\t//注意加 try catch 块\n    Strategy strategy = contextFactory.getStrategy(strategyStr); //获取策略对象\n    strategy.execute(); //执行具体操作\n}catch (Exception e) {\n    e.printStackTrace();\n}\n...\n```\n\n### 3. 状态模式\n待补充","tags":["Java","代码风格"]},{"title":"解决 jpa 的 save 过程用 null 值覆盖数据库有效值的问题","url":"/2019/07/19/解决-jpa-的-save-过程用-null-值覆盖数据库有效值的问题/","content":"\n最近在项目中使用 jpa 操作数据库的时候，遇到了一个问题。\n\n我所使用的数据库是 postgreSQL，通过定义实体类添加`@Table`注解指定表，定义接口继承`CrudRepository`类，将实体类作为参数传入，即可调用默认的 save 方法，jpa 会根据主键将实体进行插入或更新操作。\n\n但是，如果用于更新的实体中含有未赋值的属性，即值为 null 的情况时，数据库中对应字段的值会被 null 覆盖。\n\n我先是在 Google 查询了解决方案，有网友提出可以在实体类加上`@DynamicUpdate`和`@DynamicInsert`两个注解，序列化时即可忽略为 null 的值。我尝试了这种方法，启动单元测试进行了一次更新操作，却发现数据库的数据依然被 null 覆盖。\n\n为了寻找原因，我查看了这两个注解的源码，它们是在 hebernate-core 里定义的。使用 idea 的全局搜索找到了注解的实现代码,还没来得及看具体实现，却发现它被加上了`@Deprecated`注解。虽然理论上对使用是无影响的，但既然作者不推荐了，还是放弃使用吧。\n\n实在不想在苦苦寻找方法了，决定自己造个轮子，实现 update 的功能。逻辑非常简单，每次 save 之前先从数据库查找出对应数据存入实体A，将含有更新数据的实体B中的非空属性赋值给实体A，最后用实体 A 存回数据库。具体代码如下：\n\n```\n\t/**\n     * 防止 jpa 将为 null 的属性更新（因为注解DynamicUpdate无效）\n     *\n     * @param entity 包含需要更新字段的实体\n     * @param oldEntity 从数据库获取的旧实体\n     * @return 最终往数据库插入的实体\n     */\n     \n    public static <T> T updateEntity(T entity,T oldEntity){\n        Class clazz = entity.getClass();\n        Field[] fields = clazz.getDeclaredFields();\n        for (Field field : fields){\n            try {\n                PropertyDescriptor pd = new PropertyDescriptor(field.getName(), clazz);\n                Method readMethod = pd.getReadMethod();\n                Method writeMethod= pd.getWriteMethod();\n                if (readMethod.invoke(entity)!=null){   //不为null 的覆盖\n                    writeMethod.invoke(oldEntity,readMethod.invoke(entity));\n                }\n            }catch (Exception e){\n                log.error(e.getMessage());\n            }\n        }\n        return oldEntity;\n    }\n```\n\n利用反射+泛型写了个通用方法，将新旧实体作为参数传入，反射取出新实体中非 null 的值赋给旧实体，最后将旧实体返回。将方法封装成工具类方便调用。\n\n在代码中，每次更新数据库前调用一下工具，用返回的实体作为 save 对象，经测试，成功解决 null 覆盖问题。","tags":["Java","jpa","postgreSQL"]}]